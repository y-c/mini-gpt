{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d29ca056-66ac-4fe2-a595-cc2c9e941690",
   "metadata": {},
   "source": [
    "# Training GPT on Shakespeare\n",
    "\n",
    "Time to bring our model to life by training it on text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b0605ba-ed48-47b0-9302-8dc6b71f396c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load our GPT implementation (copy from previous notebook)\n",
    "# ... (include all the previous code here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d3c95-5d78-4b00-b689-519559380040",
   "metadata": {},
   "source": [
    "## Load and Prepare Shakespeare Data\n",
    "\n",
    "We'll use character-level tokenization for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2ad94fe-7941-4c9f-a7bb-0e5e475a223f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1,115,394 characters\n",
      "First 200 characters:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "\n",
      "Vocabulary size: 65\n",
      "Characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "\n",
      "Tokenization test:\n",
      "Original: Hello World!\n",
      "Encoded: [20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42, 2]\n",
      "Decoded: Hello World!\n"
     ]
    }
   ],
   "source": [
    "# Load the Shakespeare text\n",
    "with open('../data/tiny_shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Dataset length: {len(text):,} characters\")\n",
    "print(f\"First 200 characters:\\n{text[:200]}\")\n",
    "\n",
    "# Get all unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"\\nVocabulary size: {vocab_size}\")\n",
    "print(f\"Characters: {''.join(chars)}\")\n",
    "\n",
    "# Create character to index mappings\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}  # string to index\n",
    "itos = {i: ch for i, ch in enumerate(chars)}  # index to string\n",
    "\n",
    "# Tokenize functions\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(tokens):\n",
    "    return ''.join([itos[i] for i in tokens])\n",
    "\n",
    "# Test tokenization\n",
    "test_string = \"Hello World!\"\n",
    "encoded = encode(test_string)\n",
    "decoded = decode(encoded)\n",
    "print(f\"\\nTokenization test:\")\n",
    "print(f\"Original: {test_string}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b18ed67-58af-41cb-ab0b-a4565f9bcaf8",
   "metadata": {},
   "source": [
    "## Create Training and Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fed6a0b0-35f8-4c37-8496-9d5f68858f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded dataset shape: torch.Size([1115394])\n",
      "Train size: 1,003,854\n",
      "Val size: 111,540\n",
      "\n",
      "Batch shapes - X: torch.Size([4, 8]), Y: torch.Size([4, 8])\n",
      "\n",
      "Example from batch:\n",
      "Input:   may los\n",
      "Target: may lose\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Encoded dataset shape: {data.shape}\")\n",
    "\n",
    "# Split into train and validation\n",
    "n = int(0.9 * len(data))  # 90% train, 10% val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Train size: {len(train_data):,}\")\n",
    "print(f\"Val size: {len(val_data):,}\")\n",
    "\n",
    "# Create data loader\n",
    "def get_batch(split, batch_size=32, block_size=128):\n",
    "    \"\"\"Generate a batch of inputs and targets\"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # Random starting positions\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # Get sequences\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "# Test batch generation\n",
    "x, y = get_batch('train', batch_size=4, block_size=8)\n",
    "print(f\"\\nBatch shapes - X: {x.shape}, Y: {y.shape}\")\n",
    "\n",
    "# Show an example\n",
    "print(\"\\nExample from batch:\")\n",
    "print(f\"Input:  {decode(x[0].tolist())}\")\n",
    "print(f\"Target: {decode(y[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c198d87c-e2d1-4f50-8c5e-f11db96f5f26",
   "metadata": {},
   "source": [
    "## Initialize Model and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0397a0e1-2497-48c7-a610-fc4c6223eab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 10,697,472\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 256  # context length\n",
    "learning_rate = 3e-4\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "\n",
    "# Initialize model\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=384,      # Small model\n",
    "    n_heads=6,\n",
    "    n_layers=6,\n",
    "    max_len=block_size,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {total_params:,}\")\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddc767a-ca85-48e8-8fd1-744e6ca240d8",
   "metadata": {},
   "source": [
    "## Training Loop with Loss Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9539cbb-8c1e-4c9a-897e-277f3e49deb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 1/5000 [29:04<2422:40:17, 1744.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0: train loss 3.6353, val loss 3.6677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▌                                                                            | 501/5000 [1:48:37<130:34:01, 104.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 500: train loss 2.1655, val loss 2.1882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▍                                                                 | 1001/5000 [5:08:47<3511:28:28, 3161.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1000: train loss 1.6847, val loss 1.8418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████▏                                                          | 1501/5000 [13:12:34<99:06:25, 101.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1500: train loss 1.4792, val loss 1.6842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 2001/5000 [14:31:52<301:26:15, 361.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2000: train loss 1.3728, val loss 1.6030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 2501/5000 [15:08:24<80:21:59, 115.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2500: train loss 1.3123, val loss 1.5621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████▍                                 | 3001/5000 [17:13:45<62:13:33, 112.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3000: train loss 1.2615, val loss 1.5349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████▊                         | 3501/5000 [17:52:07<67:35:32, 162.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3500: train loss 1.2221, val loss 1.5251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▊                         | 3535/5000 [17:53:43<1:08:04,  2.79s/it]"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"Estimate loss on train and val sets\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size, block_size)\n",
    "            \n",
    "            # Create causal mask\n",
    "            seq_len = X.shape[1]\n",
    "            mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).to(device)\n",
    "            \n",
    "            logits = model(X, mask)\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = Y.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for iter in tqdm(range(max_iters)):\n",
    "    # Sample batch\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Create causal mask\n",
    "    seq_len = xb.shape[1]\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model(xb, mask)\n",
    "    B, T, C = logits.shape\n",
    "    logits = logits.view(B*T, C)\n",
    "    targets = yb.view(B*T)\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient clipping (important for stability!)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluate periodically\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        train_losses.append(losses['train'])\n",
    "        val_losses.append(losses['val'])\n",
    "        print(f\"\\nStep {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9536f94b-f494-42f0-b42c-ada42161ef90",
   "metadata": {},
   "source": [
    "## Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bfc258-07b7-4fb5-8bf2-376d6be656e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "steps = np.arange(0, max_iters, eval_interval)\n",
    "if len(steps) < len(train_losses):\n",
    "    steps = np.append(steps, max_iters-1)\n",
    "\n",
    "plt.plot(steps, train_losses, label='Train Loss')\n",
    "plt.plot(steps, val_losses, label='Val Loss')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.title('GPT Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final val loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3564d03e-a7e3-412d-b395-606b7ff20e9a",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "Now the fun part - let's generate some Shakespeare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8aad94c3-e9c3-4d94-877e-5688563c7de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated samples:\n",
      "==================================================\n",
      "\n",
      "Prompt: \n",
      "\n",
      "Generated: \n",
      "s\n",
      "kfD'xI$?y $fYNxnH$lHXdcn:;o!skSSqlxjENWabQlqZ!$OUYESl- GUXLhH 3tNZYHNUFYJOxXaSu?KwR&DOcce$LLDB3St O&Bw3!tKR!tSxqfwhhmJHTiWJIBaKHh$jdi;N,BOx;hfnnf3X&xdHHEvy'x?yR3M:s,TSEiETCBBDwUUf$wGh&Y?BZ,vmLtawCiX\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: ROMEO:\n",
      "Generated: ROMEO:dkmxEy.lEgigwfd,kBZuMYxEMHZ,Y,qS,!waEuUfd!HaxUbYLFt d:HttTdNd?ZOatQEd;'NN'nmnROTNHl'HkVnVCvLyRLV&sw$t;SSdNOBBhN:in$p?dwdKBFROJ&t$lXqvJ!BhwOSxlSw3JIE.h&ESdatZJNhd&Wq:UnWaTQ$aZyOBHk?tn3:,cfHs:HU!M,w,xcW\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: To be or not to be\n",
      "Generated: To be or not to beoZK!Xg?sUXa,$JEMWwk:bXbikN:jdqKEwBYw KWIYTJdgwf'EOqEUZlaGRftZXQguuNuuEG,NBXJZvHwUV g;xrg$xBHBdJ!Ewv&x: JRfIUOY !SBKEB$HdnDOUd?daEiOcahlhOdlxYq!qd:XaBVCTxdUwCfNw?fqEO:UZBSPLwGt&Ylpsywat gfUy3bUTO?eY3aU\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: What is\n",
      "Generated: What isUUZEd3D.U?ox;vTNE.XiddZnJaEM.SlHqloWimWH$!cS3TQYqJEWlUUlUSWWftZVlXhUBXED?targ3cOOWtMw s&MEgXOLYdLIYYnEpUi &OH,ScZLXR?xMF,BHUMU?$TGBwx!;H?J?W?TZpf,f,Zy,Wf$l!eVJYiYfwwxHq:dw:InJdVNfSGBCBfvUta :CRqdO,HNI\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "    \"\"\"\n",
    "    Generate text from the model\n",
    "    idx: initial context tokens [batch_size, seq_len]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop context to block_size\n",
    "        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "        \n",
    "        # Get predictions\n",
    "        seq_len = idx_cond.shape[1]\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).to(device)\n",
    "        logits = model(idx_cond, mask)\n",
    "        \n",
    "        # Focus on last time step\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        # Optional top-k sampling\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        \n",
    "        # Sample from distribution\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Append to sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx\n",
    "\n",
    "# Generate from different prompts\n",
    "prompts = [\n",
    "    \"\\n\",  # Empty prompt\n",
    "    \"ROMEO:\",\n",
    "    \"To be or not to be\",\n",
    "    \"What is\"\n",
    "]\n",
    "\n",
    "print(\"Generated samples:\\n\" + \"=\"*50)\n",
    "for prompt in prompts:\n",
    "    # Encode prompt\n",
    "    context = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    generated = generate(model, context, max_new_tokens=200, temperature=0.8, top_k=40)\n",
    "    \n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated: {decode(generated[0].tolist())}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a048b63e-15fb-43f1-a0bf-2b6a629b8d40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
