# Mini-GPT: Learn by Building

A hands-on project to learn transformer architecture by building a GPT model from scratch.

## ğŸ¯ Project Goals

1. **Implement transformer architecture from first principles** - No copying, learn by coding each component
2. **Experience real training challenges** - Debug loss spikes, handle OOM errors, fix convergence issues  
3. **Build intuition through experimentation** - Understand why certain hyperparameters matter
4. **Create a foundation for multimodal models** - Extensible to vision-language models later

## ğŸš€ Learning Approach

**Philosophy**: Learn by building, not by watching. Every concept is learned through implementation and debugging.

## ğŸ“‹ Project Phases

### Phase 1: Build Core Components (3-4 hours) âœ…
- [x] Scaled dot-product attention âœ…
- [x] Multi-head attention âœ…
- [x] Transformer block with residual connections âœ…
- [x] Full GPT architecture âœ…

### Phase 2: Make It Train (2 hours)
- [ ] Data preparation (character-level tokenization)
- [ ] Training loop with loss tracking
- [ ] Text generation with sampling
- [ ] Debug common training issues

### Phase 3: Experiment & Learn (2 hours)
- [ ] Learning rate ablations
- [ ] Architecture comparisons (depth vs width)
- [ ] Training dynamics visualization
- [ ] Memory profiling

### Future Extension: Multimodal (Optional)
- [ ] Simple vision encoder
- [ ] Interleaved image-text data
- [ ] Baby multimodal tasks

## ğŸ“ Success Criteria

- âœ… Working GPT that generates coherent text
- âœ… Documented training curves from at least 3 experiments
- âœ… One novel finding about training dynamics
- âœ… Understanding of memory/compute bottlenecks

## ğŸ› ï¸ Tech Stack

- **PyTorch** - Core implementation
- **Matplotlib** - Visualizations
- **NumPy** - Data handling
- **Weights & Biases** (optional) - Experiment tracking

## ğŸ“ Repository Structure

```
mini-gpt/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_attention_mechanism.ipynb
â”‚   â”œâ”€â”€ 02_transformer_block.ipynb
â”‚   â”œâ”€â”€ 03_training_loop.ipynb
â”‚   â””â”€â”€ 04_experiments.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model.py          # GPT implementation
â”‚   â”œâ”€â”€ train.py          # Training utilities
â”‚   â””â”€â”€ generate.py       # Text generation
â”œâ”€â”€ data/
â”‚   â””â”€â”€ tiny_shakespeare.txt
â”œâ”€â”€ checkpoints/          # Saved models
â””â”€â”€ results/             # Logs, plots, findings
```

## ğŸƒ Getting Started

```bash
# Clone the repository
git clone https://github.com/yourusername/mini-gpt.git
cd mini-gpt

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install torch numpy matplotlib

# Start with the first notebook
jupyter notebook notebooks/01_attention_mechanism.ipynb
```

## ğŸ“ Implementation Checklist

### Attention Mechanism
- [ ] Implement scaled dot-product attention
- [ ] Test with toy examples
- [ ] Visualize attention weights
- [ ] Understand masking for autoregressive models

### Multi-Head Attention  
- [ ] Project to Q, K, V
- [ ] Split heads correctly
- [ ] Concatenate and project output
- [ ] Verify shapes at each step

### Transformer Block
- [ ] Add feedforward network
- [ ] Implement layer normalization
- [ ] Add residual connections
- [ ] Test gradient flow

### Training
- [ ] Create data loader
- [ ] Implement teacher forcing
- [ ] Add gradient clipping
- [ ] Log training metrics

## ğŸ› Common Issues & Solutions

| Problem | Likely Cause | Solution |
|---------|--------------|----------|
| Loss is NaN | Attention scores too large | Add scaling factor: 1/âˆšd_k |
| Loss plateaus | Learning rate too low | Try increasing by 10x |
| Repeating tokens | Sampling temperature = 0 | Add temperature > 0 |
| OOM error | Batch size too large | Reduce batch size or use gradient accumulation |

## ğŸ“Š Experiments to Try

1. **Learning Rate Sweep**: Compare 1e-3, 1e-4, 1e-5
2. **Model Size**: 100K vs 1M parameters  
3. **Depth vs Width**: 4 layers Ã— 128d vs 2 layers Ã— 256d
4. **Position Encoding**: Learned vs sinusoidal

## ğŸ“ Key Learnings

### Attention Mechanism Insights

1. **Core Intuition**: Attention is a learnable way to decide which parts of input to focus on
   - Queries ask "what am I looking for?"
   - Keys provide "what information is available?"
   - Values contain "what to actually use"

2. **Critical Implementation Details**:
   - Scaling by âˆšd_k prevents gradient vanishing (discovered via experiment)
   - Causal masking enforces autoregressive property for generation
   - Multi-head design allows learning different types of relationships

3. **Findings from Implementation**:
   - Without scaling: attention scores std=8.03 â†’ training fails
   - With scaling: std=1.00 â†’ stable training
   - Attention weights visualization shows model literally learning what to "look at"

### Transformer Block Insights

1. **Residual Connections are CRITICAL**:
   - Without residuals: gradient norm = 0.000086 (vanished!)
   - With residuals: gradient norm = 211.58
   - **2.5 million times stronger gradient flow!**
   - This enables training very deep networks (GPT-3 has 96 layers)

2. **Component Interactions**:
   - Layer Norm stabilizes activations across features
   - Feed-forward adds "thinking" capacity between attention steps
   - Dropout prevents overfitting
   - Pre-LN architecture (normalize first) is more stable

## ğŸ‰ Progress Log

- **Session 1**: Implemented core attention mechanisms
  - âœ… Scaled dot-product attention 
  - âœ… Causal masking for autoregressive models
  - âœ… Multi-head attention with 8 heads
  - âœ… Discovered importance of âˆšd_k scaling through experiments

- **Session 2**: Built transformer block
  - âœ… Feed-forward network with GELU activation
  - âœ… Layer normalization for stability
  - âœ… Residual connections (discovered 2.5MÃ— gradient improvement!)
  - âœ… Complete transformer block with all components integrated

## ğŸ“š Resources

- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - Visual guide (reference only)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html) - For implementation details
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original paper (optional)

## ğŸ“ˆ Learning Milestones

- [ ] "I understand why we scale attention scores"
- [ ] "I can explain what each attention head learns"  
- [ ] "I know why we need position embeddings"
- [ ] "I can debug a transformer from scratch"

## ğŸ¤ Contributing

This is a personal learning project, but feel free to:
- Open issues for questions
- Share your training results
- Suggest experiments

## ğŸ“œ License

MIT - Use this for your own learning!

---

**Remember**: The goal isn't to build the best GPT, it's to understand how GPTs work by building one yourself. Every bug is a learning opportunity!