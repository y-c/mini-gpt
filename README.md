# Mini-GPT: Learn by Building

A hands-on project to learn transformer architecture by building a GPT model from scratch.

## ğŸ¯ Project Goals

1. **Implement transformer architecture from first principles** - No copying, learn by coding each component
2. **Experience real training challenges** - Debug loss spikes, handle OOM errors, fix convergence issues  
3. **Build intuition through experimentation** - Understand why certain hyperparameters matter
4. **Create a foundation for multimodal models** - Extensible to vision-language models later

## ğŸš€ Learning Approach

**Philosophy**: Learn by building, not by watching. Every concept is learned through implementation and debugging.

## ğŸ“‹ Project Phases

### Phase 1: Build Core Components (3-4 hours)
- [x] Scaled dot-product attention âœ…
- [x] Multi-head attention âœ…
- [ ] Transformer block with residual connections
- [ ] Full GPT architecture

### Phase 2: Make It Train (2 hours)
- [ ] Data preparation (character-level tokenization)
- [ ] Training loop with loss tracking
- [ ] Text generation with sampling
- [ ] Debug common training issues

### Phase 3: Experiment & Learn (2 hours)
- [ ] Learning rate ablations
- [ ] Architecture comparisons (depth vs width)
- [ ] Training dynamics visualization
- [ ] Memory profiling

### Future Extension: Multimodal (Optional)
- [ ] Simple vision encoder
- [ ] Interleaved image-text data
- [ ] Baby multimodal tasks

## ğŸ“ Success Criteria

- âœ… Working GPT that generates coherent text
- âœ… Documented training curves from at least 3 experiments
- âœ… One novel finding about training dynamics
- âœ… Understanding of memory/compute bottlenecks

## ğŸ› ï¸ Tech Stack

- **PyTorch** - Core implementation
- **Matplotlib** - Visualizations
- **NumPy** - Data handling
- **Weights & Biases** (optional) - Experiment tracking

## ğŸ“ Repository Structure

```
mini-gpt/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_attention_mechanism.ipynb
â”‚   â”œâ”€â”€ 02_transformer_block.ipynb
â”‚   â”œâ”€â”€ 03_training_loop.ipynb
â”‚   â””â”€â”€ 04_experiments.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model.py          # GPT implementation
â”‚   â”œâ”€â”€ train.py          # Training utilities
â”‚   â””â”€â”€ generate.py       # Text generation
â”œâ”€â”€ data/
â”‚   â””â”€â”€ tiny_shakespeare.txt
â”œâ”€â”€ checkpoints/          # Saved models
â””â”€â”€ results/             # Logs, plots, findings
```

## ğŸƒ Getting Started

```bash
# Clone the repository
git clone https://github.com/yourusername/mini-gpt.git
cd mini-gpt

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install torch numpy matplotlib

# Start with the first notebook
jupyter notebook notebooks/01_attention_mechanism.ipynb
```

## ğŸ“ Implementation Checklist

### Attention Mechanism
- [ ] Implement scaled dot-product attention
- [ ] Test with toy examples
- [ ] Visualize attention weights
- [ ] Understand masking for autoregressive models

### Multi-Head Attention  
- [ ] Project to Q, K, V
- [ ] Split heads correctly
- [ ] Concatenate and project output
- [ ] Verify shapes at each step

### Transformer Block
- [ ] Add feedforward network
- [ ] Implement layer normalization
- [ ] Add residual connections
- [ ] Test gradient flow

### Training
- [ ] Create data loader
- [ ] Implement teacher forcing
- [ ] Add gradient clipping
- [ ] Log training metrics

## ğŸ› Common Issues & Solutions

| Problem | Likely Cause | Solution |
|---------|--------------|----------|
| Loss is NaN | Attention scores too large | Add scaling factor: 1/âˆšd_k |
| Loss plateaus | Learning rate too low | Try increasing by 10x |
| Repeating tokens | Sampling temperature = 0 | Add temperature > 0 |
| OOM error | Batch size too large | Reduce batch size or use gradient accumulation |

## ğŸ“Š Experiments to Try

1. **Learning Rate Sweep**: Compare 1e-3, 1e-4, 1e-5
2. **Model Size**: 100K vs 1M parameters  
3. **Depth vs Width**: 4 layers Ã— 128d vs 2 layers Ã— 256d
4. **Position Encoding**: Learned vs sinusoidal

## ğŸ‰ Minimum Viable Success

Your first goal: Generate *any* text that contains real words after training for 10 minutes on your laptop. Even "the the the cat" counts as success!

## ğŸ“š Resources

- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - Visual guide (reference only)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html) - For implementation details
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original paper (optional)

## ğŸ“ˆ Learning Milestones

- [ ] "I understand why we scale attention scores"
- [ ] "I can explain what each attention head learns"  
- [ ] "I know why we need position embeddings"
- [ ] "I can debug a transformer from scratch"

## ğŸ¤ Contributing

This is a personal learning project, but feel free to:
- Open issues for questions
- Share your training results
- Suggest experiments

## ğŸ“œ License

MIT - Use this for your own learning!

---

**Remember**: The goal isn't to build the best GPT, it's to understand how GPTs work by building one yourself. Every bug is a learning opportunity!